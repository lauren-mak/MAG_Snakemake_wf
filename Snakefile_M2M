# MAG_MAG_COMPARISON

import os
from os.path import basename, join
import sys
import glob
import pandas as pd
import csv
import copy

# Directory structure
processed_dir = "processed_reports"
final_rep_dir = "final_reports"

LIST_DIR = "/athena/masonlab/scratch/users/lam4003/MIAB_MAG_Lists"
if not os.path.exists("logs"):
    os.makedirs("logs")

# LOAD METADATA
PT_ID = 'X' # The participant ID
BINNERS = ['B', 'H'] # B = MetaWRAP (standard binning), H = Proximeta (Hi-C)
METRICS = ['fastani', 'mash']
BIN_DIRS = {
    'B': "/athena/masonlab/scratch/users/lam4003/MIAB_Illumina_" + PT_ID + "/data/02_binning/coassembly/S_coas/refined_bins_50_10", 
    'H': "/athena/masonlab/scratch/users/lam4003/MIAB_Illumina_Proximeta/" + PT_ID}


rule all:
    input:
        expand(join(final_rep_dir, PT_ID + "_{metric}_matches.csv"), metric=METRICS),
        expand(join(final_rep_dir, PT_ID + "_{metric}_distances.csv"), metric=METRICS),
        expand(join(final_rep_dir, PT_ID + "_{metric}_sumstats.csv"), metric=METRICS)


rule make_mag_list:
    input:
        lambda wildcards: BIN_DIRS[wildcards.binner],
    output:
        join(LIST_DIR, PT_ID + "_{binner}.txt"),
    params:
        list_dir = LIST_DIR,
    shell:
        """
        mkdir -p {params.list_dir}
        echo {input}
        find {input} -type f > {output}
        sed -i '/snakemake/d' {output}
        """


rule finish_mag_lists: # Since fastani relies on multiple MAG lists, must generate them all
    input:
        expand(join(LIST_DIR, PT_ID + "_{binner}.txt"), binner=BINNERS)
    output:
        "mag_lists.txt",
    shell:
        """
        touch {output}
        """


###############
### FastANI ###
###############

def get_other_mags(wildcards):
    tmp = list(BINNERS)
    tmp.remove(wildcards.binner)
    return join(LIST_DIR, PT_ID + "_" + tmp[0] + ".txt"),

rule fastani:
    input:
        "mag_lists.txt",
        q=join(LIST_DIR, PT_ID + "_{binner}.txt"),
    output:
        report=join("fastani", PT_ID + "_{binner}_raw_output.tsv"),
        matrix=join("fastani", PT_ID + "_{binner}_raw_output.tsv.matrix"),
    params:
        r = get_other_mags,
    threads: workflow.cores,
    shell:
        """
        fastANI -t {threads} --queryList {input.q} --refList {params.r} -o {output.report} --matrix # {output}.matrix
        """


rule fastani_make_bms:
    input:
        fni=join("fastani", PT_ID + "_{binner}_raw_output.tsv"),
        lst=join(LIST_DIR, PT_ID + "_{binner}.txt"),
    output:
        join(processed_dir, PT_ID + "_{binner}_fastani.csv"), # [tool,bin_X,NA,dist_X]
    params:
        binner = "{binner}",
    run:
        df = pd.read_csv(str(input.fni), sep = '\t', header = None).iloc[:,[0,1,2]] # Get rid of the last two columns
        df.iloc[:,0] = ['_'.join(basename(i).replace('_', '.').split('.')[:-1]) for i in df.iloc[:,0]] # Get rid of absolute path and '.fasta'
        df.iloc[:,1] = ['_'.join(basename(i).replace('_', '.').split('.')[:-1]) for i in df.iloc[:,1]] # Get rid of absolute path and '.fasta'
        best_matches = {}
        for i, row in df.iterrows(): # Keep only matches that are the best matches ANI-wise
            if row[0] not in best_matches:
                best_matches[row[0]] = [row[1], row[2]]
            else: 
                if row[2] > best_matches[row[0]][1]:
                    best_matches[row[0]] = [row[1], row[2]] # Replace only if this match is more similar ANI-wise
        all_bins_raw = open(input.lst, 'r').read().split('\n')[:-1]
        all_bins_fin = ['_'.join(basename(i).replace('_', '.').split('.')[:-1]) for i in all_bins_raw]
        unmatched_bins = [b for b in all_bins_fin if b not in best_matches.keys()] # List of no-match bins
        for b in unmatched_bins: best_matches[b] = ["NA","NA"]
        out_dct = {'0': [], '1': [], '2': []}
        for b in best_matches:
            out_dct['0'] += [b]
            out_dct['1'] += [best_matches[b][0]]
            out_dct['2'] += [best_matches[b][1]]
        out_df = pd.DataFrame(out_dct)
        out_df.insert(loc = 0, column = "binner", value = params["binner"], allow_duplicates = True) # Add tool name as first column
        out_df.to_csv(str(output), index = False, header = False)


rule fastani_make_dst:
    input:
        expand(join("fastani", PT_ID + "_{binner}_raw_output.tsv.matrix"), binner=BINNERS),
    output:
        join(final_rep_dir, PT_ID + "_fastani_distances.csv"), # [dist_X,dist_Y,...]        
    run:
        fin_dst = []
        for i in input:
            raw_dst = open(i, 'r').read().replace('\n','\t').split('\t')[1:-1]
            fin_dst += [float(i) for i in raw_dst if i != 'NA' and 'fa' not in i]
        csv.writer(open(str(output), 'a')).writerow(fin_dst)


###############
#### Mash #####
###############


rule mash_sketch:
    input:
        join(LIST_DIR, PT_ID + "_{binner}.txt"),
    output:
        join("mash", PT_ID + "_{binner}.msh"),
    shell:
        """
        mash sketch -l {input} -o {output}
        """


rule finish_mash_lists: # Since Mash relies on multiple MAG sketches, must generate them all
    input:
        expand(join("mash", PT_ID + "_{binner}.msh"), binner=BINNERS)
    output:
        "mash_lists.txt",
    shell:
        """
        touch {output}
        """


def get_other_sketch(wildcards):
    tmp = list(BINNERS)
    tmp.remove(wildcards.binner)
    return join("mash", PT_ID + "_" + tmp[0] + ".msh"),

rule mash_dist:
    input:
        "mash_lists.txt",
        q=join("mash", PT_ID + "_{binner}.msh"),
    output:
        join("mash", PT_ID + "_{binner}.tsv"),
    params:
        r = get_other_sketch,
    threads: workflow.cores,
    shell:
        """
        mash dist -s 1000000 -p {threads} {params.r} {input.q} > {output}
        """


rule mash_make_bms: 
    input:
        join("mash", PT_ID + "_{binner}.tsv"),
    output:
        join(processed_dir, PT_ID + "_{binner}_mash.csv"), # [tool,bin_X,NA,dist_X]
    params:
        binner = "{binner}",
    run:
        df = pd.read_csv(str(input), sep = '\t', header = None).iloc[:,[0,1,2]] # Get rid of the last two columns [ref_bin, quer_bin, dist]
        df.iloc[:,0] = ['_'.join(basename(i).replace('_', '.').split('.')[:-1]) for i in df.iloc[:,0]] # Get rid of absolute path and '.fasta'
        df.iloc[:,1] = ['_'.join(basename(i).replace('_', '.').split('.')[:-1]) for i in df.iloc[:,1]] # Get rid of absolute path and '.fasta'
        best_matches = []
        for b in df.iloc[:,1].unique(): # For each unique query bin...
            query_df = df[df[1] == b]
            match_df = query_df[query_df[2] != 1] # Remove all rows with distance == 1
            if match_df.empty: # If dataframe is empty, best_matches[b] = ["NA","NA"]
                best_matches.append(pd.Series([b, "NA", "NA"]))
            else: 
                mndst_df = match_df[match_df[2] == match_df[2].min()] # Find the minimum Mash distances
                bmtch_df = mndst_df.iloc[0,:] # Take the first minimum Mash distance bin
                format_df = bmtch_df[[1,0,2]].reset_index(drop=True) # Switch order of reference and query
                best_matches.append(format_df) 
        out_df = pd.DataFrame(best_matches)
        out_df.insert(loc = 0, column = "binner", value = params["binner"], allow_duplicates = True) # Add tool name as first column
        out_df.to_csv(str(output), index = False, header = False)


rule mash_make_dst:
    input:
        expand(join("mash", PT_ID + "_{binner}.tsv"), binner=BINNERS),
    output:
        join(final_rep_dir, PT_ID + "_mash_distances.csv"), # [dist_X,dist_Y,...]        
    run:
        fin_dst = []
        for i in input:
            raw_df = pd.read_csv(i, sep = '\t', header = None)
            df = raw_df[raw_df[2] != 1] 
            fin_dst += list(df[2]) # All considered Mash distances
        csv.writer(open(str(output), 'a')).writerow(fin_dst)


###############
## Summaries ##
###############


rule find_reciprocals:
    input:
        lambda wildcards: expand(join(processed_dir, PT_ID + "_{binner}_{metric}.csv"), binner=BINNERS, metric=wildcards.metric), # How to incorporate two wildcards while only expanding one of them
    output:
        join(final_rep_dir, PT_ID + "_{metric}_matches.csv"),
    run:
        df_lst = []
        for i in input:
            df = pd.read_csv(str(i), header = None)
            df.columns = ["binner","first","second","dist"]
            df_lst += [df]
        first_bm_dct = dict(df_lst[0].iloc[:,1:3].values) # Make dictionaries of first_bin : second_bin
        secnd_bm_dct = dict(df_lst[1].iloc[:,1:3].values)
        matches_lst = [] # Reciprocal distance list
        discrep_lst = [] # No-match and non-reciprocal list
        for b in list(first_bm_dct):
            if first_bm_dct[b] in secnd_bm_dct: # If second_value in second dictionary:
                if b == secnd_bm_dct[first_bm_dct[b]]: # If first_key == second_key : first_value:
                    matches_lst += [df_lst[0][df_lst[0]['first'] == b]] # Add rows of first and second keys to reciprocal distance list
                    matches_lst += [df_lst[1][df_lst[1]['first'] == first_bm_dct[b]]]
                    secnd_bm_dct.pop(first_bm_dct[b])
                    first_bm_dct.pop(b)
                else:
                    discrep_lst += [df_lst[0][df_lst[0]['first'] == b]] # Add first_key row to the discrepancy list
            else:
                discrep_lst += [df_lst[0][df_lst[0]['first'] == b]] # Add first_key row to the discrepancy list
        for b in secnd_bm_dct:
            discrep_lst += [df_lst[1][df_lst[1]['first'] == b]] # Add second_key row to the discrepancy list
        matches_df = pd.concat(matches_lst)
        matches_df.insert(loc = 4, column = "reciprocal", value = True, allow_duplicates = True) # Reciprocal matches
        discrep_df = pd.concat(discrep_lst)
        discrep_df.insert(loc = 4, column = "reciprocal", value = False, allow_duplicates = True) # Non-reciprocal or no-match
        pd.concat([matches_df,discrep_df]).to_csv(str(output), index = False) 


def calc_sumstats(df, b): # 'Num_Bins', 'Reciprocal', 'Non_reciprocal', 'No_match'
    num_bins = df.shape[0] # Number of rows
    lst = [['Num_Bins', b, num_bins, 'NA']]
    bad_df = df[df['reciprocal'] == False]
    num_recip = num_bins - bad_df.shape[0]
    lst.append(['Reciprocal', b, num_recip, float(num_recip/num_bins)])
    num_nonrec = bad_df[bad_df['dist'].notnull()].shape[0]
    lst.append(['Non_reciprocal', b, num_nonrec, float(num_nonrec/num_bins)])
    num_nomtch = bad_df.shape[0] - num_nonrec
    lst.append(['No_match', b, num_nomtch, float(num_nomtch/num_bins)])
    return pd.DataFrame(lst)

rule summary_statistics:
    input:
        join(final_rep_dir, PT_ID + "_{metric}_matches.csv"),
    output:
        join(final_rep_dir, PT_ID + "_{metric}_sumstats.csv"),
    run:
        df = pd.read_csv(str(input), header = 0)
        metawrap_df = calc_sumstats(df[df['binner'] == 'B'], 'MetaWRAP')
        proximet_df = calc_sumstats(df[df['binner'] == 'H'], 'Proximeta')
        all_df = pd.concat([metawrap_df, proximet_df])
        all_df.columns = ["Statistic","Binning_Algorithm","Count","Proportion"]
        all_df.Statistic = pd.Categorical(all_df.Statistic,categories=['Num_Bins', 'Reciprocal', 'Non_reciprocal', 'No_match'])
        out_df = all_df.sort_values('Statistic')
        out_df.to_csv(str(output), index = False) 

